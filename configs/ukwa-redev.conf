{
    "warc" : {
        "title": "UKWA configuration",
        # Indexing configuration:
        "index" : {
            # What to extract:
            "extract" : {
                # Maximum payload size allowed to be kept wholly in RAM:
                "inMemoryThreshold" : 10M,
                # Maximum payload size that will be serialised out to disk instead of held in RAM:
                "onDiskThreshold" : 100M,
                
                # Content to extract
                "content" : {
                    # Should we index the content body text?
                    "text" : true,
                    
                    # Should we store the text, or only index it?
                    "text_stored" : true,
                    
                    # Extract UK Postcodes and geoindex?
                    "text_extract_postcodes": true,

                    # Run simple AFINN sentiment analysis?                    
                    "test_sentimentj": false,
                     
                    # Run the Stanford NER?
                    "text_stanford_ner": false,
                    
                    # Should we extract the fuzzy hash of the text?
                    "text_fuzzy_hash" : true,
                    
                    # Extract list of elements used in HTML:
                    "elements_used" : true,
                    
                    # Extract potential PDF problems using Apache PDFBox Preflight:
                    "extractApachePreflightErrors" : false,
    
                    # Extract image features:
                    "images" : {
                        "enabled" : false,
                        "maxSizeInBytes" : 1M,
                    	"detectFaces" : true,
                    	"dominantColours" : true,
                        # The random sampling rate:
                        # (where '1' means 'extract from all images', 
                        # and '100' would mean 'extract from 1 out of every 100 images')
                        "analysisSamplingRate": 50
                    }
                    
                    # Extract the first bytes of the file (for shingling):
                    "first_bytes" : {
                        # Enabled?
                        "enabled" : false,
                        # Number of bytes to extract (>=4 to allow content_ffb to work):
                        "num_bytes" : 32
                    }
                },
                
                # Which linked entities to extract:
                "linked" : {
                    "resources" : false,
                    "hosts" : false,
                    "domains" : true
                },
                
                # Restrict record types:
                "record_type_include" : [
                    response,
                    revisit
                ],
                
                # Restrict response codes:
                # works by matches starting with the characters, so "2" will match 2xx:
                "response_include" : [
                    "2"
                ],
                
                # Restrict protocols:
                "protocol_include" : [
                    http,
                    https
                ],
                
                # URLs to skip, e.g. ['robots.txt'] ???
                "url_exclude" : [],
            },
            
            # Parameters relating to format identification:   
            "id" : {
                # Allow tools to infer format from the resource URI (file extension):
                "useResourceURI" : true
    
                # DROID-specific config:
                "droid" : {
                    "enabled" : true,
                    "useBinarySignaturesOnly" : false
                },
            },
            
            # Parameters to control Apache Tika behaviour
            "tika" : {
                # Maximum length of text to extract:
                "max_text_length" : 2M,
                # Use the 'boilerpipe' text extractor rather than the usual one:
                "use_boilerpipe" : false,
                # The parse timeout (for when Tika gets stuck):
                "parse_timeout" : 300000,
                # Extract absolutely all metadata fields into a generic facet:
                "extract_all_metadata": false,
                # Formats to avoid processing
                "exclude_mime" : [
                ]
            },
            
            # Parameters to control the exclusion of results from the indexing process:
            "exclusions" : {
                # Exclusion enabled?
                "enabled" : false,
                # Default check interval before reloading the exclusions file, in seconds:
                "check_interval" : 600,
                # Exclusion URI/SURT prefix file:
                "file" : "/path/to/exclude.txt"
            }
        },
        
        # Solr configuration:
        "solr" : {
            # Use the hash+url as the ID for the documents, rather than separate documents per WARC record
            "use_hash_url_id": false,
            # Check SOLR for duplicates during indexing:
            "check_solr_for_duplicates": false
            # Server configuration:
            "servers" : "http://devsolr-proxy.n1.wa.bl.uk:8983/solr/dev",
            "collection": "dev",
            # Index to HDFS?
            "hdfs": false,
            "num_shards": 2,
            "hdfs_output_path": "ukwa_shards_1",
            # Solr document batch size for submissions:
            "batch_size" : 100,
            # Number of threads per Solr client:
            "num_threads" : 1,
            # Is this a dummy-run? (i.e. should we NOT post to SOLR?)
            "dummy_run" : false,
        },
        
        # HTTP Proxy to use when talking to Solr (if any):
        "http_proxy" : {
        },
        
        # Hadoop configuration        
        "hadoop" : {
            "num_reducers" : 40
        }
    }
}
