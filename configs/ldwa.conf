{
    #  WARC configuration namespace:
    "warc" : {
        "hadoop" : {
            "num_reducers" : 10
        },
        #  Indexing configuration:
        "index" : {
            #  Parameters to control Apache Tika behaviour
            "tika" : {
                #  Maximum length of text to extract:
                "max_text_length" : "1024K",
                #  The parse timeout (for when Tika gets stuck):
                "parse_timeout" : 300000,
                #  Formats to avoid processing
                "exclude_mime" : [
                    "x-tar",
                    "x-gzip",
                    "bz",
                    "lz",
                    "compress",
                    "zip",
                    "javascript",
                    "css",
                    "octet-stream",
                    "image",
                    "video",
                    "audio"
                ]
            },
            #  Parameters relating to format identification:   
            "id" : {
                #  DROID-specific config:
                "droid" : {
                    "enabled" : false,
                    "useBinarySignaturesOnly" : false
                },
                #  Allow tools to infer format from the resource URI (file extension):
                "useResourceURI" : false
            },
            #  Parameters to control the exclusion of results from the indexing process:
            "exclusions" : {
                #  Exclusion enabled?
                "enabled" : false,
                #  Default check interval before reloading the exclusions file, in seconds:
                "check_interval" : 600,
                #  Exclusion URI/SURT prefix file:
                "file" : "/path/to/exclude.txt"
            },
            #  What to extract:
            "extract" : {
                #  Content to extract
                "content" : {
                    #  Extract list of elements used in HTML:
                    "elements_used" : false,
                    #  Should we index the content body text?
                    "text" : true,
                    #  Extract the first bytes of the file (for shingling):
                    "first_bytes" : {
                        #  Enabled?
                        "enabled" : false,
                        #  Number of bytes to extract (>=4 to allow content_ffb to work):
                        "num_bytes" : 32
                    }
                },
                #  Restrict protocols:
                "protocol_include" : [
                    "http",
                    "https"
                ],
                #  Restrict response codes:
                #  works by matches starting with the characters, so "2" will match 2xx:
                "response_include" : [
                    "2"
                ],
                #  URLs to skip:
                # url_exclude: [robots.txt,.rss,panaccess-mime.types,.js,.cat,.css]
                "url_exclude" : [],
                #  Which linked entities to extract:
                "linked" : {
                    "resources" : false,
                    "domains" : false,
                    "hosts" : false
                }
            }
        },
        #  SolrServer
        #  Solr configuration - CURRENTLY IGNORED BY THE NON-HADOOP INDEXER -
        "solr" : {
            #  Is this a dummy-run? (i.e. should we NOT post to SOLR?)
            "dummy_run" : false,
            "zookeeper" : "192.168.1.175:9983,192.168.1.176:9983,192.168.1.177:9983",
            "server" : "http://192.168.1.203:8983/solr/ukdomain",
            "collection" : "ldwa",
            "batch_size" : 50,
            "num_threads" : 1
        },
        "act" : {
            "url" : "http://www.webarchive.org.uk/act/websites/export/all"
        },
        #  HTTP Proxy to use when talking to Solr (if any):
        "http_proxy" : {}
    }
}
